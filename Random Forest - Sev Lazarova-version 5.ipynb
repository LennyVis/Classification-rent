{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data set\n",
    "\n",
    "The first step before running the clustering algorithm is to prepare the training and the testing data set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.palplot(sns.color_palette(\"RdBu\", n_colors=7))\n",
    "\n",
    "fileNameTrain = \"C:\\\\Users\\\\sevda\\\\Documents\\\\Data Lab\\\\Six sigma rental property\\\\train.json\\\\train.json\"\n",
    "train_df = pd.read_json(fileNameTrain)\n",
    "\n",
    "fileNameTest = \"C:\\\\Users\\\\sevda\\\\Documents\\\\Data Lab\\\\Six sigma rental property\\\\test.json\\\\test.json\"\n",
    "test_df = pd.read_json(fileNameTest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we will extract the key words from the description variable - by key words, we define words that are in the description of the unit but are not stop words as defined by the ntlk.corpus package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "description_key_words_ls = []\n",
    "\n",
    "for ind, row in train_df.iterrows():\n",
    "        #print(row['features'])\n",
    "        #description = description.lower()\n",
    "        description = row['description'].lower().rstrip(',?!.')\n",
    "        description = ' '.join([word for word in description.split() if word not in cachedStopWords])\n",
    "        description_ls = description.split(\" \")\n",
    "        description_key_words_ls += [description_ls]\n",
    "\n",
    "train_df['description_key_words'] = pd.Series(description_key_words_ls, index=train_df.index)\n",
    "\n",
    "description_key_words_ls = []\n",
    "\n",
    "for ind, row in test_df.iterrows():\n",
    "        #print(row['features'])\n",
    "        #description = description.lower()\n",
    "        description = row['description'].lower().rstrip(',?!.')\n",
    "        description = ' '.join([word for word in description.split() if word not in cachedStopWords])\n",
    "        description_ls = description.split(\" \")\n",
    "        description_key_words_ls += [description_ls]\n",
    "\n",
    "test_df['description_key_words'] = pd.Series(description_key_words_ls, index=test_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create two numeric variables which describe the number of features and number of key words in the description section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['num_features'] = train_df.features.apply(len)\n",
    "train_df['num_key_words_description'] = train_df.description_key_words.apply(len)\n",
    "\n",
    "test_df['num_features'] = test_df.features.apply(len)\n",
    "test_df['num_key_words_description'] = test_df.description_key_words.apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Created variable, we will extract into new variables the exact data when the listing was created, the day of year, week of year, weekday and hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "train_df[\"date\"]= train_df[\"created\"].dt.date\n",
    "\n",
    "train_df[\"dayofyear\"] = train_df[\"created\"].dt.dayofyear\n",
    "train_df[\"weekofyear\"] = train_df[\"created\"].dt.weekofyear\n",
    "train_df[\"weekday\"] = train_df[\"created\"].dt.weekday\n",
    "train_df[\"hour\"] = train_df[\"created\"].dt.hour\n",
    "\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "test_df[\"date\"]= test_df[\"created\"].dt.date\n",
    "\n",
    "test_df[\"dayofyear\"] = test_df[\"created\"].dt.dayofyear\n",
    "test_df[\"weekofyear\"] = test_df[\"created\"].dt.weekofyear\n",
    "test_df[\"weekday\"] = test_df[\"created\"].dt.weekday\n",
    "test_df[\"hour\"] = test_df[\"created\"].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add the number of photos of each listing as a new variable in the training and testing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another step is to create two variables which describe the price per bathroom and price per bedroom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df[\"price_per_bathroom\"] = train_df[\"price\"]/(train_df[\"bathrooms\"] + 1)\n",
    "train_df[\"price_per_bedroom\"] = train_df[\"price\"]/(train_df[\"bedrooms\"] + 1)\n",
    "\n",
    "test_df[\"price_per_bathroom\"] = test_df[\"price\"]/(test_df[\"bathrooms\"] + 1)\n",
    "test_df[\"price_per_bedroom\"] = test_df[\"price\"]/(test_df[\"bedrooms\"] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the manager id as an indicator of how much interest there may be in a listing based on who is managing the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "lbl.fit(list(train_df['building_id'].values) + list(test_df['building_id'].values))\n",
    "train_df['building_id'] = lbl.transform(list(train_df['building_id'].values))\n",
    "\n",
    "test_df['building_id'] = lbl.transform(list(test_df['building_id'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some buildings are more often seen in rental ads than others. We will introduce a number of few variables which describe how often the building is been seen in a rental post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step in the data preparation, we will add the neighbourhood as another variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shapefile\n",
    "\n",
    "sf = shapefile.Reader(\"C:\\\\Users\\\\sevda\\\\Documents\\\\Data Lab\\\\Six sigma rental property\\\\ZillowNeighborhoods-NY\\\\ZillowNeighborhoods-NY.shp\")\n",
    "\n",
    "shapes = sf.shapes()\n",
    "records = sf.records()\n",
    "\n",
    "towns_values = [records[i][2] for i in range(len(records))]\n",
    "neighb_values = [records[i][3] for i in range(len(records))]\n",
    "west_values = [shapes[i].bbox[0] for i in range(len(records))]\n",
    "south_values = [shapes[i].bbox[1] for i in range(len(records))]\n",
    "east_values = [shapes[i].bbox[2] for i in range(len(records))]\n",
    "north_values = [shapes[i].bbox[3] for i in range(len(records))]\n",
    "\n",
    "west, south, east, north = -74.02, 40.64, -73.85, 40.86\n",
    "\n",
    "neighbourhood_pd = pd.DataFrame({'Town' : towns_values,\n",
    "                                 'Neighbourhood' : neighb_values,\n",
    "                                 'West' : west_values,\n",
    "                                 'South' : south_values,\n",
    "                                 'East' : east_values,\n",
    "                                 'North' : north_values})\n",
    "\n",
    "neighbourhood_pd = neighbourhood_pd[neighbourhood_pd.Town == \"New York\"]\n",
    "neighbourhood_pd = neighbourhood_pd.ix[(neighbourhood_pd.West >= west) & \n",
    "                                     (neighbourhood_pd.East <= east) & \n",
    "                                     (neighbourhood_pd.South >= south) & \n",
    "                                     (neighbourhood_pd.North <= north)]\n",
    "\n",
    "neighbourhood_sorted_pd = neighbourhood_pd.sort_values(['West'])\n",
    "\n",
    "\n",
    "neighbourhood_ls = []\n",
    "for num in range(0, train_df.shape[0]):\n",
    "    temp = neighbourhood_sorted_pd[(neighbourhood_sorted_pd.West<train_df.longitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.East>train_df.longitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.South<train_df.latitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.North>train_df.latitude.values[num])]\n",
    "    if temp.shape[0] > 0:\n",
    "        neighbourhood_ls += [temp.Neighbourhood.values[0]]\n",
    "    else:\n",
    "        neighbourhood_ls += [\"Other\"]\n",
    "    \n",
    "train_df['neighbourhood'] = pd.Series(neighbourhood_ls, index=train_df.index)\n",
    "\n",
    "neighbourhood_ls = []\n",
    "for num in range(0, test_df.shape[0]):\n",
    "    temp = neighbourhood_sorted_pd[(neighbourhood_sorted_pd.West<test_df.longitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.East>test_df.longitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.South<test_df.latitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.North>test_df.latitude.values[num])]\n",
    "    if temp.shape[0] > 0:\n",
    "        neighbourhood_ls += [temp.Neighbourhood.values[0]]\n",
    "    else:\n",
    "        neighbourhood_ls += [\"Other\"]\n",
    "    \n",
    "test_df['neighbourhood'] = pd.Series(neighbourhood_ls, index=test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop([\"price_median_x\", \"price_median_y\", \"price_diff\"], axis=1)\n",
    "test_df = test_df.drop([\"price_median_x\", \"price_median_y\", \"price_diff\"], axis=1)\n",
    "\n",
    "train_group_by = train_df[[\"bedrooms\", \"neighbourhood\", \"price\"]].groupby([\"bedrooms\", \"neighbourhood\"]).median()\n",
    "train_group_by = train_group_by.add_suffix('_median').reset_index()\n",
    "\n",
    "train_df = train_df.merge(train_group_by, left_on=['bedrooms', 'neighbourhood'], right_on=['bedrooms', 'neighbourhood'], how='inner')\n",
    "test_df = test_df.merge(train_group_by, left_on=['bedrooms', 'neighbourhood'], right_on=['bedrooms', 'neighbourhood'], how='inner')\n",
    "\n",
    "train_df[\"price_diff\"] = train_df[\"price\"] - train_df[\"price_median\"]\n",
    "test_df[\"price_diff\"] = test_df[\"price\"] - test_df[\"price_median\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bathrooms' 'bedrooms' 'building_id' 'created' 'description'\n",
      " 'display_address' 'features' 'interest_level' 'latitude' 'listing_id'\n",
      " 'longitude' 'manager_id' 'photos' 'price' 'street_address'\n",
      " 'description_key_words' 'num_features' 'num_key_words_description' 'date'\n",
      " 'dayofyear' 'weekofyear' 'weekday' 'hour' 'num_photos'\n",
      " 'price_per_bathroom' 'price_per_bedroom' 'neighbourhood' 'price_median'\n",
      " 'price_diff']\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df.drop([\"price_median_x\", \"price_median_x\", \"price\"], axis=1)\n",
    "#test_df.drop([\"price_median_x\", \"price_median_x\", \"price\"], axis=1)\n",
    "\n",
    "train_group_by = train_df[[\"bedrooms\", \"price\"]].groupby([\"bedrooms\"]).median()\n",
    "train_group_by = train_group_by.add_suffix('_median').reset_index()\n",
    "\n",
    "train_df = train_df.merge(train_group_by, left_on=['bedrooms'], right_on=['bedrooms'], how='inner')\n",
    "test_df = test_df.merge(train_group_by, left_on=['bedrooms'], right_on=['bedrooms'], how='inner')\n",
    "\n",
    "train_df[\"price_diff\"] = train_df[\"price\"] - train_df[\"price_median\"]\n",
    "test_df[\"price_diff\"] = test_df[\"price\"] - test_df[\"price_median\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use a variable describing which neighbourhood the unit is in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the variable transformations, we will apply now the Random Forest Algorithm. The algorithm is copied from https://www.kaggle.com/den3b81/two-sigma-connect-rental-listing-inquiries/improve-perfomances-using-manager-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59070397469461389"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import random\n",
    "\n",
    "selected_vars  = [\"bathrooms\", \"bedrooms\", \"price\", \"num_features\", \"num_key_words_description\",\n",
    "                   \"dayofyear\", \"weekofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\", \"longitude\",\n",
    "                    \"building_id\", \"price_diff\"]\n",
    "\n",
    "\n",
    "X = train_df[selected_vars]\n",
    "y = train_df[\"interest_level\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "random.seed(123)\n",
    "clf = RandomForestClassifier(n_estimators=1000)\n",
    "result  = clf.fit(X_train[selected_vars], y_train)\n",
    "y_val_pred = clf.predict_proba(X_val[selected_vars])\n",
    "log_loss(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore which are the most important variables used for the random forest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price' 'num_key_words_description' 'latitude' 'longitude' 'building_id'\n",
      " 'price_diff']\n"
     ]
    }
   ],
   "source": [
    "selected_vars = np.array(selected_vars)\n",
    "importances = result.feature_importances_\n",
    "important_names = selected_vars[importances > np.mean(importances)]\n",
    "print (important_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predprob = clf.predict_proba(test_df[selected_vars])\n",
    "labels2idx = {label: i for i, label in enumerate(clf.classes_)}\n",
    "\n",
    "out_df = pd.DataFrame()\n",
    "out_df[\"listing_id\"] = test_df[\"listing_id\"]\n",
    "for label in [\"high\", \"medium\", \"low\"]:\n",
    "    out_df[label] = test_predprob[:, labels2idx[label]]\n",
    "out_df.to_csv(\"random_forest_results_v5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
