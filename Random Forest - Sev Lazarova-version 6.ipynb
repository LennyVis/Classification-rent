{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data set\n",
    "\n",
    "The first step before running the clustering algorithm is to prepare the training and the testing data set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.palplot(sns.color_palette(\"RdBu\", n_colors=7))\n",
    "\n",
    "fileNameTrain = \"C:\\\\Users\\\\sevda\\\\Documents\\\\Data Lab\\\\Six sigma rental property\\\\train.json\\\\train.json\"\n",
    "train_df = pd.read_json(fileNameTrain)\n",
    "\n",
    "fileNameTest = \"C:\\\\Users\\\\sevda\\\\Documents\\\\Data Lab\\\\Six sigma rental property\\\\test.json\\\\test.json\"\n",
    "test_df = pd.read_json(fileNameTest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we will extract the key words from the description variable - by key words, we define words that are in the description of the unit but are not stop words as defined by the ntlk.corpus package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "description_key_words_ls = []\n",
    "\n",
    "for ind, row in train_df.iterrows():\n",
    "        #print(row['features'])\n",
    "        #description = description.lower()\n",
    "        description = row['description'].lower().rstrip(',?!.')\n",
    "        description = ' '.join([word for word in description.split() if word not in cachedStopWords])\n",
    "        description_ls = description.split(\" \")\n",
    "        description_key_words_ls += [description_ls]\n",
    "\n",
    "train_df['description_key_words'] = pd.Series(description_key_words_ls, index=train_df.index)\n",
    "\n",
    "description_key_words_ls = []\n",
    "\n",
    "for ind, row in test_df.iterrows():\n",
    "        #print(row['features'])\n",
    "        #description = description.lower()\n",
    "        description = row['description'].lower().rstrip(',?!.')\n",
    "        description = ' '.join([word for word in description.split() if word not in cachedStopWords])\n",
    "        description_ls = description.split(\" \")\n",
    "        description_key_words_ls += [description_ls]\n",
    "\n",
    "test_df['description_key_words'] = pd.Series(description_key_words_ls, index=test_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create two numeric variables which describe the number of features and number of key words in the description section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['num_features'] = train_df.features.apply(len)\n",
    "train_df['num_key_words_description'] = train_df.description_key_words.apply(len)\n",
    "\n",
    "test_df['num_features'] = test_df.features.apply(len)\n",
    "test_df['num_key_words_description'] = test_df.description_key_words.apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will further our analysis of the description variable by adding sentiment which is an assessment of the tone of the rental post - negative, positive or neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def description_sentiment(sentences):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        result.append(vs)\n",
    "    return pd.DataFrame(result).mean()\n",
    "\n",
    "#sdf = train_df.sample(5000)\n",
    "train_df['description_tokens'] = train_df['description'].apply(sent_tokenize)\n",
    "train_df = pd.concat([train_df,train_df['description_tokens'].apply(description_sentiment)],axis=1)\n",
    "\n",
    "test_df['description_tokens'] = test_df['description'].apply(sent_tokenize)\n",
    "test_df = pd.concat([test_df,test_df['description_tokens'].apply(description_sentiment)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Created variable, we will extract into new variables the exact data when the listing was created, the day of year, week of year, weekday and hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "train_df[\"date\"]= train_df[\"created\"].dt.date\n",
    "\n",
    "train_df[\"dayofyear\"] = train_df[\"created\"].dt.dayofyear\n",
    "train_df[\"weekofyear\"] = train_df[\"created\"].dt.weekofyear\n",
    "train_df[\"weekday\"] = train_df[\"created\"].dt.weekday\n",
    "train_df[\"hour\"] = train_df[\"created\"].dt.hour\n",
    "\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "test_df[\"date\"]= test_df[\"created\"].dt.date\n",
    "\n",
    "test_df[\"dayofyear\"] = test_df[\"created\"].dt.dayofyear\n",
    "test_df[\"weekofyear\"] = test_df[\"created\"].dt.weekofyear\n",
    "test_df[\"weekday\"] = test_df[\"created\"].dt.weekday\n",
    "test_df[\"hour\"] = test_df[\"created\"].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add the number of photos of each listing as a new variable in the training and testing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another step is to create two variables which describe the price per bathroom and price per bedroom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df[\"price_per_bathroom\"] = train_df[\"price\"]/train_df[\"bathrooms\"]\n",
    "train_df[\"price_per_bedroom\"] = train_df[\"price\"]/(train_df[\"bedrooms\"] + 1)\n",
    "\n",
    "test_df[\"price_per_bathroom\"] = test_df[\"price\"]/test_df[\"bathrooms\"]\n",
    "test_df[\"price_per_bedroom\"] = test_df[\"price\"]/(test_df[\"bedrooms\"] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the building id as a unique identifier of the building and the building itself affects the interest level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "lbl.fit(list(train_df['building_id'].values) + list(test_df['building_id'].values))\n",
    "train_df['building_id'] = lbl.transform(list(train_df['building_id'].values))\n",
    "\n",
    "test_df['building_id'] = lbl.transform(list(test_df['building_id'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will count how many rental units per building are available for rent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "building_id_group_by = train_df[[\"street_address\", \"building_id\"]].groupby([\"building_id\"]).count()\n",
    "building_id_group_by = building_id_group_by.add_suffix('_count').reset_index()\n",
    "\n",
    "train_df = train_df.merge(building_id_group_by, left_on=['building_id'], right_on=['building_id'], how='inner')\n",
    "test_df = test_df.merge(building_id_group_by, left_on=['building_id'], right_on=['building_id'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some buildings are more often seen in rental ads than others. We will introduce a number of few variables which describe how often the building is been seen in a rental post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step in the data preparation, we will explore furthere the description of the rentals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shapefile\n",
    "\n",
    "sf = shapefile.Reader(\"C:\\\\Users\\\\sevda\\\\Documents\\\\Data Lab\\\\Six sigma rental property\\\\ZillowNeighborhoods-NY\\\\ZillowNeighborhoods-NY.shp\")\n",
    "\n",
    "shapes = sf.shapes()\n",
    "records = sf.records()\n",
    "\n",
    "towns_values = [records[i][2] for i in range(len(records))]\n",
    "neighb_values = [records[i][3] for i in range(len(records))]\n",
    "west_values = [shapes[i].bbox[0] for i in range(len(records))]\n",
    "south_values = [shapes[i].bbox[1] for i in range(len(records))]\n",
    "east_values = [shapes[i].bbox[2] for i in range(len(records))]\n",
    "north_values = [shapes[i].bbox[3] for i in range(len(records))]\n",
    "\n",
    "west, south, east, north = -74.02, 40.64, -73.85, 40.86\n",
    "\n",
    "neighbourhood_pd = pd.DataFrame({'Town' : towns_values,\n",
    "                                 'Neighbourhood' : neighb_values,\n",
    "                                 'West' : west_values,\n",
    "                                 'South' : south_values,\n",
    "                                 'East' : east_values,\n",
    "                                 'North' : north_values})\n",
    "\n",
    "neighbourhood_pd = neighbourhood_pd[neighbourhood_pd.Town == \"New York\"]\n",
    "neighbourhood_pd = neighbourhood_pd.ix[(neighbourhood_pd.West >= west) & \n",
    "                                     (neighbourhood_pd.East <= east) & \n",
    "                                     (neighbourhood_pd.South >= south) & \n",
    "                                     (neighbourhood_pd.North <= north)]\n",
    "\n",
    "neighbourhood_sorted_pd = neighbourhood_pd.sort_values(['West'])\n",
    "\n",
    "\n",
    "neighbourhood_ls = []\n",
    "for num in range(0, train_df.shape[0]):\n",
    "    temp = neighbourhood_sorted_pd[(neighbourhood_sorted_pd.West<train_df.longitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.East>train_df.longitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.South<train_df.latitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.North>train_df.latitude.values[num])]\n",
    "    if temp.shape[0] > 0:\n",
    "        neighbourhood_ls += [temp.Neighbourhood.values[0]]\n",
    "    else:\n",
    "        neighbourhood_ls += [\"Other\"]\n",
    "    \n",
    "train_df['neighbourhood'] = pd.Series(neighbourhood_ls, index=train_df.index)\n",
    "\n",
    "neighbourhood_ls = []\n",
    "for num in range(0, test_df.shape[0]):\n",
    "    temp = neighbourhood_sorted_pd[(neighbourhood_sorted_pd.West<test_df.longitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.East>test_df.longitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.South<test_df.latitude.values[num]) &\n",
    "                                   (neighbourhood_sorted_pd.North>test_df.latitude.values[num])]\n",
    "    if temp.shape[0] > 0:\n",
    "        neighbourhood_ls += [temp.Neighbourhood.values[0]]\n",
    "    else:\n",
    "        neighbourhood_ls += [\"Other\"]\n",
    "    \n",
    "test_df['neighbourhood'] = pd.Series(neighbourhood_ls, index=test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df = train_df.drop([\"price_median_x\", \"price_median_y\", \"price_diff\"], axis=1)\n",
    "#test_df = test_df.drop([\"price_median_x\", \"price_median_y\", \"price_diff\"], axis=1)\n",
    "\n",
    "train_group_by = train_df[[\"bedrooms\", \"neighbourhood\", \"price\"]].groupby([\"bedrooms\", \"neighbourhood\"]).median()\n",
    "train_group_by = train_group_by.add_suffix('_median').reset_index()\n",
    "\n",
    "train_df = train_df.merge(train_group_by, left_on=['bedrooms', 'neighbourhood'], right_on=['bedrooms', 'neighbourhood'], how='inner')\n",
    "test_df = test_df.merge(train_group_by, left_on=['bedrooms', 'neighbourhood'], right_on=['bedrooms', 'neighbourhood'], how='inner')\n",
    "\n",
    "train_df[\"price_diff\"] = train_df[\"price\"] - train_df[\"price_median\"]\n",
    "test_df[\"price_diff\"] = test_df[\"price\"] - test_df[\"price_median\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Random Forest function to run the model. We will apply the algorithm to the train data set with the pre selected variables and target variable \"interest_level\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_vars  = [\"price\", \"price_diff\", \"num_features\", \"num_key_words_description\",\n",
    "                   \"dayofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\", \"longitude\",\n",
    "                    \"price_per_bedroom\", \"price_per_bathroom\", \"neg\", \"neu\", \"street_address_count\"]\n",
    "\n",
    "X = train_df[selected_vars]\n",
    "y = train_df[\"interest_level\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000)\n",
    "clf.fit(X_train[selected_vars], y_train)\n",
    "y_val_pred = clf.predict_proba(X_val[selected_vars])\n",
    "log_loss(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predprob = clf.predict_proba(test_df[selected_vars])\n",
    "labels2idx = {label: i for i, label in enumerate(clf.classes_)}\n",
    "labels2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame()\n",
    "out_df[\"listing_id\"] = test_df[\"listing_id\"]\n",
    "for label in [\"high\", \"medium\", \"low\"]:\n",
    "    out_df[label] = test_predprob[:, labels2idx[label]]\n",
    "out_df.to_csv(\"random_forest_results_v2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
