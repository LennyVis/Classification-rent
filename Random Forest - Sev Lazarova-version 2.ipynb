{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data set\n",
    "\n",
    "The first step before running the clustering algorithm is to prepare the training and the testing data set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.palplot(sns.color_palette(\"RdBu\", n_colors=7))\n",
    "\n",
    "fileNameTrain = \"C:\\\\Users\\\\sevda\\\\Documents\\\\Data Lab\\\\Six sigma rental property\\\\train.json\\\\train.json\"\n",
    "train_df = pd.read_json(fileNameTrain)\n",
    "\n",
    "fileNameTest = \"C:\\\\Users\\\\sevda\\\\Documents\\\\Data Lab\\\\Six sigma rental property\\\\test.json\\\\test.json\"\n",
    "test_df = pd.read_json(fileNameTest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we will extract the key words from the description variable - by key words, we define words that are in the description of the unit but are not stop words as defined by the ntlk.corpus package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "description_key_words_ls = []\n",
    "\n",
    "for ind, row in train_df.iterrows():\n",
    "        #print(row['features'])\n",
    "        #description = description.lower()\n",
    "        description = row['description'].lower().rstrip(',?!.')\n",
    "        description = ' '.join([word for word in description.split() if word not in cachedStopWords])\n",
    "        description_ls = description.split(\" \")\n",
    "        description_key_words_ls += [description_ls]\n",
    "\n",
    "train_df['description_key_words'] = pd.Series(description_key_words_ls, index=train_df.index)\n",
    "\n",
    "description_key_words_ls = []\n",
    "\n",
    "for ind, row in test_df.iterrows():\n",
    "        #print(row['features'])\n",
    "        #description = description.lower()\n",
    "        description = row['description'].lower().rstrip(',?!.')\n",
    "        description = ' '.join([word for word in description.split() if word not in cachedStopWords])\n",
    "        description_ls = description.split(\" \")\n",
    "        description_key_words_ls += [description_ls]\n",
    "\n",
    "test_df['description_key_words'] = pd.Series(description_key_words_ls, index=test_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create two numeric variables which describe the number of features and number of key words in the description section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['num_features'] = train_df.features.apply(len)\n",
    "train_df['num_key_words_description'] = train_df.description_key_words.apply(len)\n",
    "\n",
    "test_df['num_features'] = test_df.features.apply(len)\n",
    "test_df['num_key_words_description'] = test_df.description_key_words.apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Created variable, we will extract into new variables the exact data when the listing was created, the day of year, week of year, weekday and hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "train_df[\"date\"]= train_df[\"created\"].dt.date\n",
    "\n",
    "train_df[\"dayofyear\"] = train_df[\"created\"].dt.dayofyear\n",
    "train_df[\"weekofyear\"] = train_df[\"created\"].dt.weekofyear\n",
    "train_df[\"weekday\"] = train_df[\"created\"].dt.weekday\n",
    "train_df[\"hour\"] = train_df[\"created\"].dt.hour\n",
    "\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "test_df[\"date\"]= test_df[\"created\"].dt.date\n",
    "\n",
    "test_df[\"dayofyear\"] = test_df[\"created\"].dt.dayofyear\n",
    "test_df[\"weekofyear\"] = test_df[\"created\"].dt.weekofyear\n",
    "test_df[\"weekday\"] = test_df[\"created\"].dt.weekday\n",
    "test_df[\"hour\"] = test_df[\"created\"].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add the number of photos of each listing as a new variable in the training and testing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"price_per_bathroom\"] = train_df[\"price\"]/train_df[\"bathrooms\"]\n",
    "train_df[\"price_per_bedroom\"] = train_df[\"price\"]/train_df[\"bedrooms\"]\n",
    "\n",
    "test_df[\"price_per_bathroom\"] = test_df[\"price\"]/test_df[\"bathrooms\"]\n",
    "test_df[\"price_per_bedroom\"] = test_df[\"price\"]/test_df[\"bedrooms\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "lbl.fit(list(train_df['building_id'].values) + list(test_df['building_id'].values))\n",
    "train_df['building_id'] = lbl.transform(list(train_df['building_id'].values))\n",
    "\n",
    "test_df['building_id'] = lbl.transform(list(test_df['building_id'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the variable transformations, we will apply now the Random Forest Algorithm. The algorithm is copied from https://www.kaggle.com/den3b81/two-sigma-connect-rental-listing-inquiries/improve-perfomances-using-manager-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60201421286196022"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_vars  = [\"bathrooms\", \"bedrooms\", \"price\", \"num_features\", \"num_key_words_description\",\n",
    "                   \"dayofyear\", \"weekofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\", \"longitude\",\n",
    "                    \"building_id\"]\n",
    "\n",
    "\n",
    "X = train_df[selected_vars]\n",
    "y = train_df[\"interest_level\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000)\n",
    "clf.fit(X_train[selected_vars], y_train)\n",
    "y_val_pred = clf.predict_proba(X_val[selected_vars])\n",
    "log_loss(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'high': 0, 'low': 1, 'medium': 2}\n"
     ]
    }
   ],
   "source": [
    "test_predprob = clf.predict_proba(test_df[selected_vars])\n",
    "labels2idx = {label: i for i, label in enumerate(clf.classes_)}\n",
    "labels2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame()\n",
    "out_df[\"listing_id\"] = test_df[\"listing_id\"]\n",
    "for label in [\"high\", \"medium\", \"low\"]:\n",
    "    out_df[label] = test_predprob[:, labels2idx[label]]\n",
    "out_df.to_csv(\"random_forest_results_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
