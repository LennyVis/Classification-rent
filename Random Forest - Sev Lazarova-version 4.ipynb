{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data set\n",
    "\n",
    "The first step before running the clustering algorithm is to prepare the training and the testing data set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.palplot(sns.color_palette(\"RdBu\", n_colors=7))\n",
    "\n",
    "fileNameTrain = \"C:\\\\Users\\\\sevda\\\\Documents\\\\Data Lab\\\\Six sigma rental property\\\\train.json\\\\train.json\"\n",
    "train_df = pd.read_json(fileNameTrain)\n",
    "\n",
    "fileNameTest = \"C:\\\\Users\\\\sevda\\\\Documents\\\\Data Lab\\\\Six sigma rental property\\\\test.json\\\\test.json\"\n",
    "test_df = pd.read_json(fileNameTest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we will extract the key words from the description variable - by key words, we define words that are in the description of the unit but are not stop words as defined by the ntlk.corpus package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "description_key_words_ls = []\n",
    "\n",
    "for ind, row in train_df.iterrows():\n",
    "        #print(row['features'])\n",
    "        #description = description.lower()\n",
    "        description = row['description'].lower().rstrip(',?!.')\n",
    "        description = ' '.join([word for word in description.split() if word not in cachedStopWords])\n",
    "        description_ls = description.split(\" \")\n",
    "        description_key_words_ls += [description_ls]\n",
    "\n",
    "train_df['description_key_words'] = pd.Series(description_key_words_ls, index=train_df.index)\n",
    "\n",
    "description_key_words_ls = []\n",
    "\n",
    "for ind, row in test_df.iterrows():\n",
    "        #print(row['features'])\n",
    "        #description = description.lower()\n",
    "        description = row['description'].lower().rstrip(',?!.')\n",
    "        description = ' '.join([word for word in description.split() if word not in cachedStopWords])\n",
    "        description_ls = description.split(\" \")\n",
    "        description_key_words_ls += [description_ls]\n",
    "\n",
    "test_df['description_key_words'] = pd.Series(description_key_words_ls, index=test_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create two numeric variables which describe the number of features and number of key words in the description section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['num_features'] = train_df.features.apply(len)\n",
    "train_df['num_key_words_description'] = train_df.description_key_words.apply(len)\n",
    "\n",
    "test_df['num_features'] = test_df.features.apply(len)\n",
    "test_df['num_key_words_description'] = test_df.description_key_words.apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Created variable, we will extract into new variables the exact data when the listing was created, the day of year, week of year, weekday and hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "train_df[\"date\"]= train_df[\"created\"].dt.date\n",
    "\n",
    "train_df[\"dayofyear\"] = train_df[\"created\"].dt.dayofyear\n",
    "train_df[\"weekofyear\"] = train_df[\"created\"].dt.weekofyear\n",
    "train_df[\"weekday\"] = train_df[\"created\"].dt.weekday\n",
    "train_df[\"hour\"] = train_df[\"created\"].dt.hour\n",
    "\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "test_df[\"date\"]= test_df[\"created\"].dt.date\n",
    "\n",
    "test_df[\"dayofyear\"] = test_df[\"created\"].dt.dayofyear\n",
    "test_df[\"weekofyear\"] = test_df[\"created\"].dt.weekofyear\n",
    "test_df[\"weekday\"] = test_df[\"created\"].dt.weekday\n",
    "test_df[\"hour\"] = test_df[\"created\"].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add the number of photos of each listing as a new variable in the training and testing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another step is to create two variables which describe the price per bathroom and price per bedroom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df[\"price_per_bathroom\"] = train_df[\"price\"]/(train_df[\"bathrooms\"] + 1)\n",
    "train_df[\"price_per_bedroom\"] = train_df[\"price\"]/(train_df[\"bedrooms\"] + 1)\n",
    "\n",
    "test_df[\"price_per_bathroom\"] = test_df[\"price\"]/(test_df[\"bathrooms\"] + 1)\n",
    "test_df[\"price_per_bedroom\"] = test_df[\"price\"]/(test_df[\"bedrooms\"] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the manager id as an indicator of how much interest there may be in a listing based on who is managing the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "lbl.fit(list(train_df['manager_id'].values) + list(test_df['manager_id'].values))\n",
    "train_df['manager_id'] = lbl.transform(list(train_df['manager_id'].values))\n",
    "\n",
    "test_df['manager_id'] = lbl.transform(list(test_df['manager_id'].values))\n",
    "\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "lbl.fit(list(train_df['building_id'].values) + list(test_df['building_id'].values))\n",
    "train_df['building_id'] = lbl.transform(list(train_df['building_id'].values))\n",
    "\n",
    "test_df['building_id'] = lbl.transform(list(test_df['building_id'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some buildings are more often seen in rental ads than others. We will introduce a number of few variables which describe how often the building is been seen in a rental post. https://www.kaggle.com/visnaga/two-sigma-connect-rental-listing-inquiries/xgboost-for-the-millionth-time-0-54724-lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buildings_count = train_df['building_id'].value_counts()\n",
    "\n",
    "train_df['top_10_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 90)] else 0)\n",
    "train_df['top_25_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 75)] else 0)\n",
    "train_df['top_5_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 95)] else 0)\n",
    "train_df['top_50_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 50)] else 0)\n",
    "train_df['top_1_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 99)] else 0)\n",
    "train_df['top_2_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 98)] else 0)\n",
    "train_df['top_15_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 85)] else 0)\n",
    "train_df['top_20_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 80)] else 0)\n",
    "train_df['top_30_building'] = train_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 70)] else 0)\n",
    "\n",
    "test_df['top_10_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 90)] else 0)\n",
    "test_df['top_25_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 75)] else 0)\n",
    "test_df['top_5_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 95)] else 0)\n",
    "test_df['top_50_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 50)] else 0)\n",
    "test_df['top_1_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 99)] else 0)\n",
    "test_df['top_2_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 98)] else 0)\n",
    "test_df['top_15_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 85)] else 0)\n",
    "test_df['top_20_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 80)] else 0)\n",
    "test_df['top_30_building'] = test_df['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 70)] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bathrooms' 'bedrooms' 'building_id' 'created' 'description'\n",
      " 'display_address' 'features' 'interest_level' 'latitude' 'listing_id'\n",
      " 'longitude' 'manager_id' 'photos' 'price' 'street_address'\n",
      " 'description_key_words' 'num_features' 'num_key_words_description' 'date'\n",
      " 'dayofyear' 'weekofyear' 'weekday' 'hour' 'num_photos'\n",
      " 'price_per_bathroom' 'price_per_bedroom' 'top_10_building'\n",
      " 'top_25_building' 'top_5_building' 'top_50_building' 'top_1_building'\n",
      " 'top_2_building' 'top_15_building' 'top_20_building' 'top_30_building']\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the variable transformations, we will apply now the Random Forest Algorithm. The algorithm is copied from https://www.kaggle.com/den3b81/two-sigma-connect-rental-listing-inquiries/improve-perfomances-using-manager-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59364727636485259"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "selected_vars  = [\"bathrooms\", \"bedrooms\", \"price\", \"num_features\", \"num_key_words_description\",\n",
    "                   \"dayofyear\", \"weekofyear\", \"weekday\", \"hour\", \"num_photos\", \"latitude\", \"longitude\",\n",
    "                    \"building_id\", \"manager_id\", \"price_per_bathroom\", \"price_per_bedroom\", \"top_1_building\", \n",
    "                    \"top_5_building\", \"top_10_building\", \"top_25_building\", \"top_50_building\"]\n",
    "\n",
    "\n",
    "X = train_df[selected_vars]\n",
    "y = train_df[\"interest_level\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000)\n",
    "result  = clf.fit(X_train[selected_vars], y_train)\n",
    "y_val_pred = clf.predict_proba(X_val[selected_vars])\n",
    "log_loss(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price' 'num_features' 'num_key_words_description' 'dayofyear' 'hour'\n",
      " 'num_photos' 'latitude' 'longitude' 'building_id' 'manager_id'\n",
      " 'price_per_bathroom' 'price_per_bedroom']\n"
     ]
    }
   ],
   "source": [
    "selected_vars = np.array(selected_vars)\n",
    "importances = result.feature_importances_\n",
    "important_names = selected_vars[importances > np.mean(importances)]\n",
    "print (important_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'high': 0, 'low': 1, 'medium': 2}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predprob = clf.predict_proba(test_df[selected_vars])\n",
    "labels2idx = {label: i for i, label in enumerate(clf.classes_)}\n",
    "labels2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame()\n",
    "out_df[\"listing_id\"] = test_df[\"listing_id\"]\n",
    "for label in [\"high\", \"medium\", \"low\"]:\n",
    "    out_df[label] = test_predprob[:, labels2idx[label]]\n",
    "out_df.to_csv(\"random_forest_results_v4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
